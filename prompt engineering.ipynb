{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a5b4f5",
   "metadata": {},
   "source": [
    "***leveraging prompting for text classification***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set device once at the top\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# # When loading models\n",
    "# model.to(device)\n",
    "\n",
    "# # When passing data\n",
    "# data = data.to(device)\n",
    "\n",
    "attn_implementation='eager'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a text generation model\n",
    "from huggingface_hub import ModelHubMixin\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model.to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "pipe= pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False   #setting this false ensures somewhat consistent output, most probable token \n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341060b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\":\"user\",\"content\":\"tell me a funny joke about chickens\"}]\n",
    "\n",
    "response=pipe(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pipe(messages, do_sample=True, temperature=1, top_p=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb667af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\":\"user\", \"content\":\"classify the text into negative or positive: the movie is amazing!\"}]\n",
    "response=pipe(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee3d72",
   "metadata": {},
   "source": [
    "look below how we modeled it up to give just one word as output-- they're called output indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c80c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\"role\":\"user\", \"content\":\"classify the text into negative or positive: text:the movie is amazing! , Sentiment:\"}]\n",
    "response=pipe(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries.\\n\"\n",
    "instruction = \"Summarize the key findings of the paper provided.\\n\"\n",
    "context = \"Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.\\n\"\n",
    "data_format = \"Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.\\n\"\n",
    "audience = \"The summary is designed for busy researchers that  quickly need to grasp the newest trends in Large Language Models.\\n\"\n",
    "tone = \"The tone should be professional and clear.\\n\"\n",
    "text = \"transfer Learning for Identifying Land Use and Land Cover from Satellite Imagery\"\n",
    "data = f\"Text to summarize: {text}\"\n",
    "# The full prompt - remove and add pieces to view its impact on the generated output\n",
    "#like for text-- with open(filename, \"r\") as f:\n",
    "#                text= f.read()\n",
    "query = persona+instruction\n",
    "response=pipe(query+ instruction + context + data_format + audience+ tone + data)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9661e07",
   "metadata": {},
   "source": [
    "**we can also provide examples of how the output must look like, demonstrated below as to how it should react to a made up word:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8804674",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_prompt= [\n",
    "    {\n",
    "    \"role\":\"user\",\"content\":\"'abhimanyu' is the name of the saviour of the universe. It might sound made up or fictional but its actually close to be proven. An example of sentence that uses word 'abhimanyu' is:\",\n",
    "    \"role\":\"assistant\",\"content\":\"abhimanyu was, is and will forever be the greatest to do it.\",\n",
    "\n",
    "    \"role\":\"user\", \"content\":\"A 'totulolu' is a spell used on bitches to behave them. An example of sentence that uses word 'totulolu' is: \"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6278c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=pipe(one_shot_prompt, temperature=1, top_p=1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a59d47",
   "metadata": {},
   "source": [
    "chain prompting--dont need to give ex pretty easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answering with chain-of-thought\n",
    "cot_prompt = [\n",
    "{\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11.The answer is 11.\"},\n",
    "{\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=pipe(cot_prompt)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623feff8",
   "metadata": {},
   "source": [
    "here, many different forms that work but\n",
    "a common and effective method is to use the phrase “Let’s think step-by-\n",
    "step,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_tot=[\n",
    "    {\n",
    "        \"role\":\"user\",\"content\":\"imagine 3 different running experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group.\\n\"\n",
    "        \"then all experts will go on to the next step, etc. If any expert realizes they are wrong, they leave the conversation. The question is: 'Will running long miles on a low protein diet \\n\"\n",
    "        \"and no weight training, lead to heavy muscle loss in an individual?' Make sure to discuss the results.\" \n",
    "        \n",
    "    }\n",
    "]\n",
    "response= pipe(zeroshot_tot)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1ee66",
   "metadata": {},
   "source": [
    "**output verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8027bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#providing examples:\n",
    "#1. without examples\n",
    "zeroshot_prompt=[\n",
    "    {\n",
    "        \"role\":\"user\",\"content\":\"create a character profile for an RPG game in JSON format\"\n",
    "    }\n",
    "]\n",
    "response=pipe(zeroshot_prompt)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 example\n",
    "one_shot_template = \"\"\"Create a short character profile for an\n",
    "RPG game. Make sure to only use this format:\n",
    "{\n",
    "\"description\": \"A SHORT DESCRIPTION\",\n",
    "\"name\": \"THE CHARACTER'S NAME\",\n",
    "\"armor\": \"ONE PIECE OF ARMOR\",\n",
    "\"weapon\": \"ONE OR MORE WEAPONS\"\n",
    "}\n",
    "\"\"\"\n",
    "one_shot_prompt = [\n",
    "{\"role\": \"user\", \"content\": one_shot_template}\n",
    "]\n",
    "# Generate the output\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dcb0e",
   "metadata": {},
   "source": [
    "although it worked here, still won't ensure proper formatting all the time.. so we move to grammer to constrained sampling\n",
    "\n",
    "**either we do validation on the output until it gets the output correct OR WE VALIDATE AT THE TIME OF TOKEN SELECTION ITSELF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------clear all outputs to free ram------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-24T11:46:15.478817Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-07-24T11:46:15.478902Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.156946832s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp.llama import Llama  #used here to get json grammer\n",
    "\n",
    "llm=Llama.from_pretrained(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    filename=\"*fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verborse=False\n",
    ")\n",
    "llm.to(device)\n",
    "\n",
    "output= llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON format\"},],\n",
    "    response_format={\"type\":\"json_object\"},\n",
    "    temperature=0,\n",
    "\n",
    ")['choices'][0]['message'][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0be8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Format as json\n",
    "json_output = json.dumps(json.loads(output), indent=4)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d94c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
