{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bf451e",
   "metadata": {},
   "source": [
    "model I/O\n",
    "\n",
    "memory\n",
    "\n",
    "retrieval\n",
    "\n",
    "agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279341d",
   "metadata": {},
   "source": [
    "Quantization reduces the number of bits required to represent the\n",
    "parameters of an LLM while attempting to maintain most of the original\n",
    "information.\n",
    "\n",
    "this reduces the precision slightly but easily makes up by much aster speed and low load on the vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ollama with Llama3\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize Ollama with Llama3\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3:latest\",  # Make sure this matches your downloaded model name\n",
    "    n_gpu_layers=-1,\n",
    "    # temperature=0.7,\n",
    "    max_tokens=500,  # Max tokens to generate\n",
    "    seed=42,\n",
    "    num_ctx=2048,\n",
    "\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c68dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 1+1 is... 2!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf779bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "you are a helpful assistant answering concisely\n",
    "<</SYS>>\n",
    "\n",
    "{user_input} [/INST]\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4320d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain= prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e82f089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_user(some_text: str):\n",
    "    response=basic_chain.invoke({\"user_input\": some_text})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2db9be15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Abhimanyu!\\n\\nThe answer to 1+1 is... 2!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_user(\"hi! I'm Abhimanyu, what is 1+1=?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8917dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae1f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
