{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bf451e",
   "metadata": {},
   "source": [
    "model I/O\n",
    "\n",
    "memory\n",
    "\n",
    "retrieval\n",
    "\n",
    "agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279341d",
   "metadata": {},
   "source": [
    "Quantization reduces the number of bits required to represent the\n",
    "parameters of an LLM while attempting to maintain most of the original\n",
    "information.\n",
    "\n",
    "this reduces the precision slightly but easily makes up by much aster speed and low load on the vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "929800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ollama with Llama3\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize Ollama with Llama3\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3:latest\",  # Make sure this matches your downloaded model name\n",
    "    n_gpu_layers=-1,\n",
    "    # temperature=0.7,\n",
    "    max_tokens=500,  # Max tokens to generate\n",
    "    seed=42,\n",
    "    num_ctx=2048,\n",
    "\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c68dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 1+1 is... 2!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4cf779bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "you are a helpful assistant answering concisely\n",
    "[/SYS]\n",
    "\n",
    "{user_input} [/INST]\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d4320d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain= prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e82f089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_user(some_text: str):\n",
    "    response=basic_chain.invoke({\"user_input\": some_text})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2db9be15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Abhimanyu!\\n\\nThe answer is: 2'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_user(\"hi! I'm Abhimanyu, what is 1+1=?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e8917dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "ntemplate=\"\"\"<s>[INST]<<SYS>>\n",
    "Create a title for a story about {summary}. Only return the title.[/SYS]\n",
    "[/INST]\"\"\"\n",
    "title_prompt=PromptTemplate(template=ntemplate, input_variable= [\"summary\"])\n",
    "\n",
    "title=LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2ae1f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'the guy sang his heart out for her',\n",
       " 'title': '\"A Melody of Devotion\"'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({ \"summary\":\"the guy sang his heart out for her\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bb9129af",
   "metadata": {},
   "outputs": [],
   "source": [
    "template= \"\"\"<s>[INST]<<SYS>>\n",
    "Describe the main character of a story about {summary} with the\n",
    "title {title}. Use only two sentences.[/SYS][/INST]\"\"\"\n",
    "\n",
    "character_prompt= PromptTemplate(template=template, input_variable=[\"summary\",\"title\"])\n",
    "character= LLMChain(llm=llm, prompt= character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1a1dcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "Create a story about {summary} with the title {title}. The main\n",
    "character is: {character}. Only return the story and it cannot be\n",
    "longer than one paragraph.[/SYS][/INST]\"\"\"\n",
    "story_prompt= PromptTemplate(template=template, input_variable= [\"summary\",\"title\",\"character\"])\n",
    "\n",
    "story= LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f11e90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain= title | character | story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c003915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'the guy sang his heart out for her',\n",
       " 'title': '\"A Melody of Devotion\"',\n",
       " 'character': 'The main character, Ethan, is a shy and introverted music teacher who pours his heart and soul into writing and performing a love song dedicated to the woman he loves, Sophia. With every note and lyric, Ethan hopes to convey the depth of his emotions and win her heart, even if it means risking rejection and hurt.',\n",
       " 'story': '<EVENT>[A Melody of Devotion]</EVENT>\\n\\nEthan stood nervously on stage, his guitar trembling in his hands as he gazed out at Sophia. The dim lights of the small coffee shop seemed to fade into the background as he began to sing his heart out, pouring every ounce of emotion into the lyrics of \"A Melody of Devotion\". His voice wavered slightly at first, but as he hit the chorus, a fierce passion took over, his words tumbling out in a rush of sincerity. \"Oh Sophia, my love for you is like a symphony, playing sweet melodies on my heartstrings,\" he sang, his eyes locked onto hers with an unspoken plea. The air was electric with tension as Sophia\\'s face transformed from surprise to wonder, her eyes welling up with tears as Ethan\\'s melody wrapped itself around her very being.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"the guy sang his heart out for her\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e493d",
   "metadata": {},
   "source": [
    "so now we have a chain and we only need to give it summary and get distintly all the elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43bb80",
   "metadata": {},
   "source": [
    "**memory** --these models are stateless rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca19bd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Abhimanyu!\\n\\nThe answer is: 2'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_user(\"hi! I'm Abhimanyu, what is 1+1=?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ceee6cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not aware of your name. You haven't provided it to me yet! Would you like to share?\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_user(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eb3f4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>your are a general assistant, be concise and to the point in your answers\n",
    "[/SYS]{chat_history}{user_input}[/INST]\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(template=template, input_variable=[\"user_input\",\"chat_history\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b2cfd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory=ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "mllm= LLMChain(llm=llm,prompt=prompt,memory=memory,output_key=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "36bfc99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': \"hi! I'm Abhimanyu, what is 1+1=?\",\n",
       " 'chat_history': '',\n",
       " 'answer': 'Nice to meet you, Abhimanyu!\\n\\nThe answer is: 2'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mllm.invoke({\"user_input\":\"hi! I'm Abhimanyu, what is 1+1=?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "425fac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'say my name?',\n",
       " 'chat_history': \"Human: hi! I'm Abhimanyu, what is 1+1=?\\nAI: Nice to meet you, Abhimanyu!\\n\\nThe answer is: 2\\nHuman: what is my name?\\nAI: Nice to meet you too, Abhimanyu!\\n\\nYour name is: Abhimanyu\",\n",
       " 'answer': '<INST>\\n<SYS>>AI: Your name is indeed \"Abhimanyu\".'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mllm.invoke({\"user_input\":\"say my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8fb9db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to only save upto k conversations in history so that we do not run out of token limit\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory=ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "mllm= LLMChain(llm=llm,prompt=prompt,memory=memory,output_key=\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48925b0",
   "metadata": {},
   "source": [
    "now the size reduces but the longer histories are traded off..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d462a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation memory\n",
    "summary_prompt_template=\"\"\"you are a summarizer\n",
    "summarise the conversations and update with the new lines.\n",
    "Current summary:\n",
    "{summary}\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "New summary: \"\"\"\n",
    "\n",
    "summary_prompt= PromptTemplate(template=summary_prompt_template, input_variables=[\"new_lines\",\"summary\"])\n",
    "\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "memory=ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", prompt=summary_prompt)\n",
    "llm_chain=LLMChain(llm=llm, prompt=prompt, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "248c3311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': \"hi! i'm abhimanyu\",\n",
       " 'chat_history': '',\n",
       " 'text': \"Hello Abhimanyu! I'm here to assist you with any queries or tasks. What can I help you with today?\"}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"hi! i'm abhimanyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "79c178db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'what is 1+1',\n",
       " 'chat_history': \"Here is the updated summary:\\n\\n**Summary:** A conversation has just started between Human (Abhimanyu) and AI. The human introduces himself as Abhimanyu, and the AI responds by introducing itself and offering assistance.\\n\\nLet me know when you're ready to add more lines to the conversation!\",\n",
       " 'text': '<s>[INST]<<SYS>>The answer to 1+1 is 2.[/SYS]'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "acc721f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'what is your name in one word',\n",
       " 'chat_history': 'I\\'m ready!\\n\\nHere\\'s the updated summary:\\n\\n**Summary:** A conversation has started between Human (Abhimanyu) and AI. The human introduces himself as Abhimanyu, and the AI responds by introducing itself and offering assistance. The human then asks a simple math question \"what is 1+1\", and the AI answers correctly that the answer is 2.\\n\\nLet me know when you\\'re ready to add more lines to the conversation!',\n",
       " 'text': '<INST>\\n\\n**Answer:** Zeta'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"what is your name in one word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3a6af6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'what is 8+11?',\n",
       " 'chat_history': 'Here\\'s the updated summary:\\n\\n**Summary:** A conversation has started between Human (Abhimanyu) and AI. The human introduces himself as Abhimanyu, and the AI responds by introducing itself as Zeta and offering assistance. The human then asks a few questions to get to know the AI better: first, he asks \"what is 1+1\", and the AI answers correctly that the answer is 2. Next, he asks for the AI\\'s name in one word, to which the AI responds with the single word \"<INST>\", which is later revealed to be Zeta.\\n\\nLet me know when you\\'re ready to add more lines to the conversation!',\n",
       " 'text': '19'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"what is 8+11?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e47d4",
   "metadata": {},
   "source": [
    "hallucinated :-:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': 'I\\'m ready!\\n\\nHere\\'s the updated summary:\\n\\n**Summary:** A conversation has started between Human (Abhimanyu) and AI (Zeta). The human introduces himself as Abhimanyu, and Zeta responds by introducing itself as Zeta and offering assistance. The human then asks a few questions to get to know the AI better: first, he asks \"what is 1+1\", and Zeta answers correctly that the answer is 2. Next, he asks for Zeta\\'s name in one word, to which Zeta responds with the single word \"<INST>\", which is later revealed to be its own name. The human then asks another math question: \"what is 8+11?\", and Zeta answers correctly that the result is 19.\\n\\nLet me know when you\\'re ready to add more lines to the conversation!'}\n"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4609f36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'what was my first question?',\n",
       " 'chat_history': 'I\\'m ready!\\n\\nHere\\'s the updated summary:\\n\\n**Summary:** A conversation has started between Human (Abhimanyu) and AI (Zeta). The human introduces himself as Abhimanyu, and Zeta responds by introducing itself as Zeta and offering assistance. The human then asks a few questions to get to know the AI better: first, he asks \"what is 1+1\", and Zeta answers correctly that the answer is 2. Next, he asks for Zeta\\'s name in one word, to which Zeta responds with the single word \"<INST>\", which is later revealed to be its own name. The human then asks another math question: \"what is 8+11?\", and Zeta answers correctly that the result is 19.\\n\\nLet me know when you\\'re ready to add more lines to the conversation!',\n",
       " 'text': 'Your first question was \"what is 1+1\"?'}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"what was my first question?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937b17b",
   "metadata": {},
   "source": [
    "**AGENTS,** \n",
    "framework: reasoning and acting ( ReAct) \n",
    "\n",
    "\n",
    "3 steps: \n",
    "\n",
    "\n",
    "Thought\n",
    "Action\n",
    "Observation\n",
    "\n",
    "the agent describes its thoughts (what it should do), its\n",
    "actions (what it will do), and its observations (the results of the action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "efdef5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import api_key\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"gemini_api_key\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCnHDSkoFg2FKuNG00nFl4f2xPeA2O5cqo\"\n",
    "gemini_llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ad49b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_template=\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format EXACTLY:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(template=react_template, input_variables=[\"tools\", \"tool_names\", \"input\",\n",
    "\"agent_scratchpad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5da5c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "import time\n",
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "# You can create the tool to pass to an agent\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(name=\"duckduck\",\n",
    "description=\"A web search engine. Use this to as a search engine for general queries.\",func=search.run)\n",
    "# Prepare tools\n",
    "tools = load_tools([\"llm-math\"], llm=gemini_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6702adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for DuckDuckGo search\n",
    "# !pip install duckduckgo-search\n",
    "\n",
    "# DuckDuckGo search is now set up with rate limiting to avoid 202 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f7d24704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nimport os\\n\\n# Set your Tavily API key\\n# os.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key_here\"\\n\\n# tavily_search = TavilySearchResults(max_results=3)\\n# search_tool = Tool(\\n#     name=\"tavily_search\",\\n#     description=\"A web search engine for current information and pricing.\",\\n#     func=tavily_search.run,\\n# )\\n'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: Use Tavily for better search (requires API key)\n",
    "# Uncomment and use this if you have a Tavily API key\n",
    "\"\"\"\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "# Set your Tavily API key\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key_here\"\n",
    "\n",
    "# tavily_search = TavilySearchResults(max_results=3)\n",
    "# search_tool = Tool(\n",
    "#     name=\"tavily_search\",\n",
    "#     description=\"A web search engine for current information and pricing.\",\n",
    "#     func=tavily_search.run,\n",
    "# )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a7d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0ad67f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "agent= create_react_agent(gemini_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True,\n",
    "    # max_iterations=5,  # Limit iterations to prevent infinite loops\n",
    "    early_stopping_method=\"generate\",  # Stop early if needed\n",
    "    return_intermediate_steps=True  # Show intermediate steps for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5663ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: duckduck\n",
      "Action Input: current price of MacBook Pro in USD\u001b[0m\u001b[32;1m\u001b[1;3mAction: duckduck\n",
      "Action Input: current price of MacBook Pro in USD\u001b[0m"
     ]
    },
    {
     "ename": "DuckDuckGoSearchException",
     "evalue": "https://html.duckduckgo.com/html 202 Ratelimit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[270], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the current price of a MacBook Pro in USD? How much would it cost in INR if the exchange rate is 90 INR for 1 USD.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m}\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    171\u001b[0m         inputs,\n\u001b[1;32m    172\u001b[0m         outputs,\n\u001b[1;32m    173\u001b[0m         return_only_outputs,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain/agents/agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1634\u001b[0m             next_step_output,\n\u001b[1;32m   1635\u001b[0m             intermediate_steps,\n\u001b[1;32m   1636\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1637\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain/agents/agent.py:1325\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1318\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1325\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   1326\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1327\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1328\u001b[0m                 color_mapping,\n\u001b[1;32m   1329\u001b[0m                 inputs,\n\u001b[1;32m   1330\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1331\u001b[0m                 run_manager,\n\u001b[1;32m   1332\u001b[0m             ),\n\u001b[1;32m   1333\u001b[0m         ),\n\u001b[1;32m   1334\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain/agents/agent.py:1408\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1408\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain/agents/agent.py:1433\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1432\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1433\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1441\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_core/tools/base.py:883\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    882\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    884\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    885\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_core/tools/base.py:852\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    851\u001b[0m         tool_kwargs \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {config_param: config}\n\u001b[0;32m--> 852\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_core/tools/simple.py:105\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    104\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_core/tools/base.py:883\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    882\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    884\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    885\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_core/tools/base.py:852\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    851\u001b[0m         tool_kwargs \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {config_param: config}\n\u001b[0;32m--> 852\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_community/tools/ddg_search/tool.py:112\u001b[0m, in \u001b[0;36mDuckDuckGoSearchResults._run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    109\u001b[0m     run_manager: Optional[CallbackManagerForToolRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Union[List[\u001b[38;5;28mdict\u001b[39m], \u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mdict\u001b[39m]]:\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     raw_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    116\u001b[0m         {\n\u001b[1;32m    117\u001b[0m             k: v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_results\n\u001b[1;32m    122\u001b[0m     ]\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:146\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.results\u001b[0;34m(self, query, max_results, source)\u001b[0m\n\u001b[1;32m    142\u001b[0m source \u001b[38;5;241m=\u001b[39m source \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    144\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    145\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     ]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    149\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    150\u001b[0m         {\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddgs_news(query, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[1;32m    158\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:64\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[0;32m---> 64\u001b[0m     ddgs_gen \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ddgs_gen]\n",
      "File \u001b[0;32m~/Desktop/geospacy/venv/lib/python3.11/site-packages/duckduckgo_search/duckduckgo_search.py:256\u001b[0m, in \u001b[0;36mDDGS.text\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    253\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError to search using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m         err \u001b[38;5;241m=\u001b[39m ex\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(err)\n",
      "\u001b[0;31mDuckDuckGoSearchException\u001b[0m: https://html.duckduckgo.com/html 202 Ratelimit"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "{\n",
    "\"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in INR if the exchange rate is 90 INR for 1 USD.\"\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e107c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Calculator\n",
      "Action Input: 25 * 4\u001b[0m\u001b[32;1m\u001b[1;3mAction: Calculator\n",
      "Action Input: 25 * 4\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 100\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 100\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 100\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Simple test result: {'input': 'What is 25 * 4?', 'output': '100', 'intermediate_steps': [(AgentAction(tool='Calculator', tool_input='25 * 4', log='Action: Calculator\\nAction Input: 25 * 4'), 'Answer: 100')]}\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 100\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Simple test result: {'input': 'What is 25 * 4?', 'output': '100', 'intermediate_steps': [(AgentAction(tool='Calculator', tool_input='25 * 4', log='Action: Calculator\\nAction Input: 25 * 4'), 'Answer: 100')]}\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple question first\n",
    "test_result = agent_executor.invoke({\n",
    "    \"input\": \"What is 25 * 4?\"\n",
    "})\n",
    "print(\"Simple test result:\", test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ac6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
